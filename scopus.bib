Scopus
EXPORT DATE: 24 July 2023

@ARTICLE{Terra2023,
	author = {Terra, Mohamed and Baklola, Mohamed and Ali, Shaimaa and El-Bastawisy, Karim},
	title = {Opportunities, applications, challenges and ethical implications of artificial intelligence in psychiatry: a narrative review},
	year = {2023},
	journal = {Egyptian Journal of Neurology, Psychiatry and Neurosurgery},
	volume = {59},
	number = {1},
	doi = {10.1186/s41983-023-00681-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163166235&doi=10.1186%2fs41983-023-00681-z&partnerID=40&md5=4d963b5d54b91d613cbdd926effd1898},
	affiliations = {Faculty of Medicine, Mansoura University, 60El-Gomhoria Street, Mansoura, 35516, Egypt},
	abstract = {Background: Artificial intelligence (AI) has made significant advances in recent years, and its applications in psychiatry have gained increasing attention. The use of AI in psychiatry offers the potential to improve patient outcomes and provide valuable insights for healthcare workers. However, the potential benefits of AI in psychiatry are accompanied by several challenges and ethical implications that require consideration. In this review, we explore the use of AI in psychiatry and its applications in monitoring mental illness, treatment, prediction, diagnosis, and deep learning. We discuss the potential benefits of AI in terms of improved patient outcomes, efficiency, and cost-effectiveness. However, we also address the challenges and ethical implications associated with the use of AI in psychiatry, including issues of accuracy, privacy, and the risk of perpetuating existing biases in the field. Results: This is a review article, thus not applicable. Conclusion: Despite the challenges and ethical implications of using AI in psychiatry, the potential benefits of this technology cannot be ignored. Further research and development are required to address the limitations and ensure the safe and ethical integration of AI in the field of psychiatry. By doing so, AI has the potential to greatly improve patient outcomes and enhance the delivery of mental healthcare services. © 2023, The Author(s).},
	author_keywords = {Artificial intelligence; Deep learning; Mental illness; Psychiatry},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Wu20231325,
	author = {Wu, Yibo and Min, Hewei and Li, Mingzi and Shi, Yuhui and Ma, Aijuan and Han, Yumei and Gan, Yadi and Guo, Xiaohui and Sun, Xinying},
	title = {Effect of Artificial Intelligence-based Health Education Accurately Linking System (AI-HEALS) for Type 2 diabetes self-management: protocol for a mixed-methods study},
	year = {2023},
	journal = {BMC public health},
	volume = {23},
	number = {1},
	pages = {1325},
	doi = {10.1186/s12889-023-16066-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164426753&doi=10.1186%2fs12889-023-16066-z&partnerID=40&md5=06d1c92a3499126b576f54c96cf27427},
	affiliations = {Department of Social Medicine and Health Education, School of Public Health, Peking University, Beijing, China; School of Nursing, Peking University, Beijing, China; Beijing Center for Disease Control and Prevention, Beijing, China; Beijing Medical Examination Center, Beijing, China; Daxing District Center for Disease Control and Prevention of Beijing, Beijing, China; Peking University First Hospital, Beijing, China},
	abstract = {BACKGROUND: Patients with type 2 diabetes (T2DM) have an increasing need for personalized and Precise management as medical technology advances. Artificial intelligence (AI) technologies on mobile devices are being developed gradually in a variety of healthcare fields. As an AI field, knowledge graph (KG) is being developed to extract and store structured knowledge from massive data sets. It has great prospects for T2DM medical information retrieval, clinical decision-making, and individual intelligent question and answering (QA), but has yet to be thoroughly researched in T2DM intervention. Therefore, we designed an artificial intelligence-based health education accurately linking system (AI-HEALS) to evaluate if the AI-HEALS-based intervention could help patients with T2DM improve their self-management abilities and blood glucose control in primary healthcare. METHODS: This is a nested mixed-method study that includes a community-based cluster-randomized control trial and personal in-depth interviews. Individuals with T2DM between the ages of 18 and 75 will be recruited from 40-45 community health centers in Beijing, China. Participants will either receive standard diabetes primary care (SDPC) (control, 3 months) or SDPC plus AI-HEALS online health education program (intervention, 3 months). The AI-HEALS runs in the WeChat service platform, which includes a KBQA, a system of physiological indicators and lifestyle recording and monitoring, medication and blood glucose monitoring reminders, and automated, personalized message sending. Data on sociodemography, medical examination, blood glucose, and self-management behavior will be collected at baseline, as well as 1,3,6,12, and 18 months later. The primary outcome is to reduce HbA1c levels. Secondary outcomes include changes in self-management behavior, social cognition, psychology, T2DM skills, and health literacy. Furthermore, the cost-effectiveness of the AI-HEALS-based intervention will be evaluated. DISCUSSION: KBQA system is an innovative and cost-effective technology for health education and promotion for T2DM patients, but it is not yet widely used in the T2DM interventions. This trial will provide evidence on the efficacy of AI and mHealth-based personalized interventions in primary care for improving T2DM outcomes and self-management behaviors. TRIAL REGISTRATION: Biomedical Ethics Committee of Peking University: IRB00001052-22,058, 2022/06/06; Clinical Trials: ChiCTR2300068952, 02/03/2023. © 2023. The Author(s).},
	author_keywords = {Artificial intelligence; Intelligent question and answering; Mixed-methods study; Mobile health; Type 2 diabetes},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Albahri2023156,
	author = {Albahri, A.S. and Duhaim, Ali M. and Fadhel, Mohammed A. and Alnoor, Alhamzah and Baqer, Noor S. and Alzubaidi, Laith and Albahri, O.S. and Alamoodi, A.H. and Bai, Jinshuai and Salhi, Asma and Santamaría, Jose and Ouyang, Chun and Gupta, Ashish and Gu, Yuantong and Deveci, Muhammet},
	title = {A systematic review of trustworthy and explainable artificial intelligence in healthcare: Assessment of quality, bias risk, and data fusion},
	year = {2023},
	journal = {Information Fusion},
	volume = {96},
	pages = {156 – 191},
	doi = {10.1016/j.inffus.2023.03.008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151265419&doi=10.1016%2fj.inffus.2023.03.008&partnerID=40&md5=ed8929df202fd77c56bb836cbd97bccd},
	affiliations = {Iraqi Commission for Computers and Informatics (ICCI), Baghdad, Iraq; Ministry of Education, ThiQar, Iraq; College of Computer, Science and Information Technology, University of Sumer, Thi Qar, Iraq; Southern Technical University, Basrah, Iraq; Ministry of Education, Baghdad, Iraq; School of Mechanical, Medical, Process Engineering, Queensland University of Technology, Brisbane, 4000, QLD, Australia; ARC Industrial Transformation Training Centre – Joint Biomechanics, Queensland University of Technology, Brisbane, 4000, QLD, Australia; Computer Techniques Engineering Department, Mazaya University College, Nasiriyah, Iraq; Department of Computer Science and Information Technology, La Trobe University, Melbourne, VIC, Australia; Faculty of Computing and Meta-Technology (FKMT), Universiti Pendidikan Sultan Idris (UPSI), Perak, Malaysia; Akunah Company for Medical Technology, Brisbane, 4120, QLD, Australia; Department of Computer Science, University of Jaén, Jaén, 23071, Spain; School of Psychology and Counselling, Queensland University of Technology, Brisbane, 4000, QLD, Australia; The Bartlett School of Sustainable Construction, University College London, 1-19 Torrington Place, London, WC1E 7HB, United Kingdom; Department of Industrial Engineering, Turkish Naval Academy, National Defence University, Istanbul, Tuzla, 34940, Turkey},
	abstract = {In the last few years, the trend in health care of embracing artificial intelligence (AI) has dramatically changed the medical landscape. Medical centres have adopted AI applications to increase the accuracy of disease diagnosis and mitigate health risks. AI applications have changed rules and policies related to healthcare practice and work ethics. However, building trustworthy and explainable AI (XAI) in healthcare systems is still in its early stages. Specifically, the European Union has stated that AI must be human-centred and trustworthy, whereas in the healthcare sector, low methodological quality and high bias risk have become major concerns. This study endeavours to offer a systematic review of the trustworthiness and explainability of AI applications in healthcare, incorporating the assessment of quality, bias risk, and data fusion to supplement previous studies and provide more accurate and definitive findings. Likewise, 64 recent contributions on the trustworthiness of AI in healthcare from multiple databases (i.e., ScienceDirect, Scopus, Web of Science, and IEEE Xplore) were identified using a rigorous literature search method and selection criteria. The considered papers were categorised into a coherent and systematic classification including seven categories: explainable robotics, prediction, decision support, blockchain, transparency, digital health, and review. In this paper, we have presented a systematic and comprehensive analysis of earlier studies and opened the door to potential future studies by discussing in depth the challenges, motivations, and recommendations. In this study a systematic science mapping analysis in order to reorganise and summarise the results of earlier studies to address the issues of trustworthiness and objectivity was also performed. Moreover, this work has provided decisive evidence for the trustworthiness of AI in health care by presenting eight current state-of-the-art critical analyses regarding those more relevant research gaps. In addition, to the best of our knowledge, this study is the first to investigate the feasibility of utilising trustworthy and XAI applications in healthcare, by incorporating data fusion techniques and connecting various important pieces of information from available healthcare datasets and AI algorithms. The analysis of the revised contributions revealed crucial implications for academics and practitioners, and then potential methodological aspects to enhance the trustworthiness of AI applications in the medical sector were reviewed. Successively, the theoretical concept and current use of 17 XAI methods in health care were addressed. Finally, several objectives and guidelines were provided to policymakers to establish electronic health-care systems focused on achieving relevant features such as legitimacy, morality, and robustness. Several types of information fusion in healthcare were focused on in this study, including data, feature, image, decision, multimodal, hybrid, and temporal. © 2023},
	author_keywords = {Artificial intelligence; Explainability; Healthcare; Information fusion; Trustworthiness},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{McKay2023,
	author = {McKay, Francis and Williams, Bethany J. and Prestwich, Graham and Bansal, Daljeet and Treanor, Darren and Hallowell, Nina},
	title = {Artificial intelligence and medical research databases: ethical review by data access committees},
	year = {2023},
	journal = {BMC Medical Ethics},
	volume = {24},
	number = {1},
	doi = {10.1186/s12910-023-00927-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164254290&doi=10.1186%2fs12910-023-00927-8&partnerID=40&md5=f86764e42598094e68fd80b4578de8dc},
	affiliations = {Population Health Sciences Institute, University of Newcastle, Newcastle Upon Tyne, NE2 4AX, United Kingdom; National Pathology Imaging Co-operative, Leeds Teaching Hospitals NHS Trust, Leeds, LS9 7TF, United Kingdom; Yorkshire and Humber Academic Health Science Network, Unit 1, Calder Close, Calder Park, Wakefield, WF4 3BA, United Kingdom; Department of Pathology, University of Leeds, Leeds, United Kingdom; Department of Clinical Pathology, Linköping University, Linköping, Sweden; Center for Medical Image Science and Visualization (CMIV), Linköping University, Linköping, Sweden; The Ethox Centre and the Wellcome Centre for Ethics and Humanities, Nuffield Department of Population Health, University of Oxford, Oxford, OX3 7LF, United Kingdom},
	abstract = {Background: It has been argued that ethics review committees—e.g., Research Ethics Committees, Institutional Review Boards, etc.— have weaknesses in reviewing big data and artificial intelligence research. For instance, they may, due to the novelty of the area, lack the relevant expertise for judging collective risks and benefits of such research, or they may exempt it from review in instances involving de-identified data. Main body: Focusing on the example of medical research databases we highlight here ethical issues around de-identified data sharing which motivate the need for review where oversight by ethics committees is weak. Though some argue for ethics committee reform to overcome these weaknesses, it is unclear whether or when that will happen. Hence, we argue that ethical review can be done by data access committees, since they have de facto purview of big data and artificial intelligence projects, relevant technical expertise and governance knowledge, and already take on some functions of ethical review. That said, like ethics committees, they may have functional weaknesses in their review capabilities. To strengthen that function, data access committees must think clearly about the kinds of ethical expertise, both professional and lay, that they draw upon to support their work. Conclusion: Data access committees can undertake ethical review of medical research databases provided they enhance that review function through professional and lay ethical expertise. © 2023, The Author(s).},
	author_keywords = {Artificial intelligence; Data access committees; Ethical expertise; Ethical review; Health data repositories; Medical research databases; Public involvement; Research ethics committees},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Ott2023,
	author = {Ott, Tabea and Heckel, Maria and Öhl, Natalie and Steigleder, Tobias and Albrecht, Nils C. and Ostgathe, Christoph and Dabrock, Peter},
	title = {Palliative care and new technologies. The use of smart sensor technologies and its impact on the Total Care principle},
	year = {2023},
	journal = {BMC Palliative Care},
	volume = {22},
	number = {1},
	doi = {10.1186/s12904-023-01174-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153905067&doi=10.1186%2fs12904-023-01174-9&partnerID=40&md5=c0d7f147c149b1cd97bd67bfa0148f2c},
	affiliations = {Chair of Systematic Theology II (Ethics), Faculty of Humanities, Social Sciences, and Theology, Friedrich-Alexander-Universität Erlangen-Nürnberg, Kochstraße 6, Erlangen, 91054, Germany; Department of Palliative Medicine, University Hospital Erlangen, Friedrich-Alexander-Universität Erlangen-Nürnberg, Werner-von-Siemens-Straße 34, Erlangen, 91052, Germany; Institute for High Frequency Technology, Hamburg University of Technology, Denickestraße 22 (I), Hamburg, 21073, Germany},
	abstract = {Background: Palliative care is an integral part of health care, which in term has become increasingly technologized in recent decades. Lately, innovative smart sensors combined with artificial intelligence promise better diagnosis and treatment. But to date, it is unclear: how are palliative care concepts and their underlying assumptions about humans challenged by smart sensor technologies (SST) and how can care benefit from SST? Aims: The paper aims to identify changes and challenges in palliative care due to the use of SST. In addition, normative guiding criteria for the use of SST are developed. Methods: The principle of Total Care used by the European Association for Palliative Care (EAPC) forms the basis for the ethical analysis. Drawing on this, its underlying conceptions of the human and its socio-ethical aspects are examined with a phenomenological focus. In the second step, the advantages, limitations, and socio-ethical challenges of using SST with respect to the Total Care principle are explored. Finally, ethical-normative requirements for the application of SST are derived. Results and Conclusion: First, SST are limited in their measurement capabilities. Second, SST have an impact on human agency and autonomy. This concerns both the patient and the caregiver. Third, some aspects of the Total Care principle are likely to be marginalized due to the use of SST. The paper formulates normative requirements for using SST to serve human flourishing. It unfolds three criteria according to which SST must be aligned: (1) evidence and purposefulness, (2) autonomy, and (3) Total Care. © 2023, The Author(s).},
	author_keywords = {Artificial Intelligence; Ethics; Quality of life; Smart Sensor Technologies; Total Care},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Vallès-Peris20231685,
	author = {Vallès-Peris, Núria and Domènech, Miquel},
	title = {Caring in the in-between: a proposal to introduce responsible AI and robotics to healthcare},
	year = {2023},
	journal = {AI and Society},
	volume = {38},
	number = {4},
	pages = {1685 – 1695},
	doi = {10.1007/s00146-021-01330-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121352390&doi=10.1007%2fs00146-021-01330-w&partnerID=40&md5=fe3117e3f09dc33472dcfd349fd22f65},
	affiliations = {Barcelona Science and Technology Studies Group (STS-B), Department of Social Psychology, Universitat Autònoma de Barcelona, Barcelona, Spain},
	abstract = {In the scenario of growing polarization of promises and dangers that surround artificial intelligence (AI), how to introduce responsible AI and robotics in healthcare? In this paper, we develop an ethical–political approach to introduce democratic mechanisms to technological development, what we call “Caring in the In-Between”. Focusing on the multiple possibilities for action that emerge in the realm of uncertainty, we propose an ethical and responsible framework focused on care actions in between fears and hopes. Using the theoretical perspective of Science and Technology Studies and empirical research, “Caring in the In-Between” is based on three movements: the first is a change of focus from the world of promises and dangers to the world of uncertainties; the second is a conceptual shift from assuming a relationship with robotics based on a Human–Robot Interaction to another focused on the network in which the robot is embedded (the “Robot Embedded in a Network”); and the last is an ethical shift from a general normative framework to a discussion on the context of use. Based on these suggestions, “Caring in the In-Between” implies institutional challenges, as well as new practices in healthcare systems. It is articulated around three simultaneous processes, each of them related to practical actions in the “in-between” dimensions considered: monitoring relations and caring processes, through public engagement and institutional changes; including concerns and priorities of stakeholders, with the organization of participatory processes and alternative forms of representation; and making fears and hopes commensurable, through the choice of progressive and reversible actions. © 2021, The Author(s).},
	author_keywords = {Care robots; Ethics of care; Measured action; Robot embedded in a network (REN); Science and technology studies},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Orlova2023749,
	author = {Orlova, I.A. and Akopyan, Zh A. and Plisyuk, A.G. and Tarasova, E.V. and Borisov, E.N. and Dolgushin, G.O. and Khvatova, E.I. and Grigoryan, M.A. and Gabbasova, L.A. and Kamalov, A.A.},
	title = {Opinion research among Russian Physicians on the application of technologies using artificial intelligence in the field of medicine and health care},
	year = {2023},
	journal = {BMC health services research},
	volume = {23},
	number = {1},
	pages = {749},
	doi = {10.1186/s12913-023-09493-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164843132&doi=10.1186%2fs12913-023-09493-6&partnerID=40&md5=05b2d650610a9113f63910ae0c953bf7},
	affiliations = {Medical Research and Education Center of Lomonosov, Moscow State University, 27/10 Lomonosov Prospect, 119192, Moscow, Russian Federation; Faculty of Fundamental Medicine, Lomonosov Moscow State University, 27/1 Lomonosov Prospect, 119192, Moscow, Russian Federation},
	abstract = {BACKGROUND: To date, no opinion surveys has been conducted among Russian physicians to study their awareness about artificial intelligence. With a survey, we aimed to evaluate the attitudes of stakeholders to the usage of technologies employing AI in the field of medicine and healthcare and identify challenges and perspectives to introducing AI. METHODS: We conducted a 12-question online survey using Google Forms. The survey consisted of questions related to the recognition of AI and attitudes towards it, the direction of development of AI in medicine and the possible risks of using AI in medicine. RESULTS: 301 doctors took part in the survey. 107 (35.6%) responded that they are familiar with AI. The vast majority of participants considered AI useful in the medical field (85%). The advantage of AI was associated with the ability to analyze huge volumes of clinically relevant data in real time (79%). Respondents highlighted areas where AI would be most useful-organizational optimization (74%), biopharmaceutical research (67%), and disease diagnosis (52%). Among the possible problems when using AI, they noted the lack of flexibility and limited application on controversial issues (64% and 60% of respondents). 56% believe that AI decision making will be difficult if inadequate information is presented for analysis. A third of doctors fear that specialists with little experience took part in the development of AI, and 89% of respondents believe that doctors should participate in the development of AI for medicine and healthcare. Only 20 participants (6.6%) responded that they agree that AI can replace them at work. At the same time, 76% of respondents believe that in the future, doctors using AI will replace those who do not. CONCLUSIONS: Russian doctors are for AI in medicine. Most of the respondents believe that AI will not replace them in the future and will become a useful tool. First of all, for optimizing organizational processes, research and diagnostics of diseases. TRIAL REGISTRATION: This study was approved by the Local Ethics Committee of the Lomonosov Moscow State University Medical Research and Education Center (IRB00010587). © 2023. The Author(s).},
	author_keywords = {AI; Artificial intelligence; Awareness; Physicians},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Valentine20231627,
	author = {Valentine, Lee and D’Alfonso, Simon and Lederman, Reeva},
	title = {Recommender systems for mental health apps: advantages and ethical challenges},
	year = {2023},
	journal = {AI and Society},
	volume = {38},
	number = {4},
	pages = {1627 – 1638},
	doi = {10.1007/s00146-021-01322-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123090528&doi=10.1007%2fs00146-021-01322-w&partnerID=40&md5=b9fdbbba210f46e96c086fd1bd7a6af5},
	affiliations = {Orygen, Parkville, 3052, VIC, Australia; Centre for Youth Mental Health, University of Melbourne, Parkville, 3010, VIC, Australia; School of Computing and Information Systems, Faculty of Engineering and Information Technology, University of Melbourne, Parkville, 3010, VIC, Australia},
	abstract = {Recommender systems assist users in receiving preferred or relevant services and information. Using such technology could be instrumental in addressing the lack of relevance digital mental health apps have to the user, a leading cause of low engagement. However, the use of recommender systems for digital mental health apps, particularly those driven by personal data and artificial intelligence, presents a range of ethical considerations. This paper focuses on considerations particular to the juncture of recommender systems and digital mental health technologies. While separate bodies of work have focused on these two areas, to our knowledge, the intersection presented in this paper has not yet been examined. This paper identifies and discusses a set of advantages and ethical concerns related to incorporating recommender systems into the digital mental health (DMH) ecosystem. Advantages of incorporating recommender systems into DMH apps are identified as (1) a reduction in choice overload, (2) improvement to the digital therapeutic alliance, and (3) increased access to personal data & self-management. Ethical challenges identified are (1) lack of explainability, (2) complexities pertaining to the privacy/personalization trade-off and recommendation quality, and (3) the control of app usage history data. These novel considerations will provide a greater understanding of how DMH apps can effectively and ethically implement recommender systems. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	author_keywords = {Artificial intelligence; Digital ethics; Digital health; Digital interventions; Ethics; Mental health; Recommender systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Zhang2023,
	author = {Zhang, Jie and Zhang, Zong-ming},
	title = {Ethics and governance of trustworthy medical artificial intelligence},
	year = {2023},
	journal = {BMC Medical Informatics and Decision Making},
	volume = {23},
	number = {1},
	doi = {10.1186/s12911-023-02103-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146278182&doi=10.1186%2fs12911-023-02103-9&partnerID=40&md5=0ab4729ed84e430bdec9d013c5f6179d},
	affiliations = {Institute of Literature in Chinese Medicine, Nanjing University of Chinese Medicine, Nanjing, 210023, China; Nantong University Xinglin College, Nantong, 226236, China; Research Center of Chinese Medicine Culture, Nanjing University of Chinese Medicine, Nanjing, 210023, China},
	abstract = {Background: The growing application of artificial intelligence (AI) in healthcare has brought technological breakthroughs to traditional diagnosis and treatment, but it is accompanied by many risks and challenges. These adverse effects are also seen as ethical issues and affect trustworthiness in medical AI and need to be managed through identification, prognosis and monitoring. Methods: We adopted a multidisciplinary approach and summarized five subjects that influence the trustworthiness of medical AI: data quality, algorithmic bias, opacity, safety and security, and responsibility attribution, and discussed these factors from the perspectives of technology, law, and healthcare stakeholders and institutions. The ethical framework of ethical values-ethical principles-ethical norms is used to propose corresponding ethical governance countermeasures for trustworthy medical AI from the ethical, legal, and regulatory aspects. Results: Medical data are primarily unstructured, lacking uniform and standardized annotation, and data quality will directly affect the quality of medical AI algorithm models. Algorithmic bias can affect AI clinical predictions and exacerbate health disparities. The opacity of algorithms affects patients’ and doctors’ trust in medical AI, and algorithmic errors or security vulnerabilities can pose significant risks and harm to patients. The involvement of medical AI in clinical practices may threaten doctors ‘and patients’ autonomy and dignity. When accidents occur with medical AI, the responsibility attribution is not clear. All these factors affect people’s trust in medical AI. Conclusions: In order to make medical AI trustworthy, at the ethical level, the ethical value orientation of promoting human health should first and foremost be considered as the top-level design. At the legal level, current medical AI does not have moral status and humans remain the duty bearers. At the regulatory level, strengthening data quality management, improving algorithm transparency and traceability to reduce algorithm bias, and regulating and reviewing the whole process of the AI industry to control risks are proposed. It is also necessary to encourage multiple parties to discuss and assess AI risks and social impacts, and to strengthen international cooperation and communication. © 2023, The Author(s).},
	author_keywords = {Algorithms; Artificial intelligence; Data; Ethics; Governance; Healthcare; Regulation; Responsibility attribution},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Suhag2023,
	author = {Suhag, Anju and Kidd, Jennifer and McGath, Meghan and Rajesh, Raeshmma and Gelfinbein, Joseph and Cacace, Nicole and Monteleone, Berrin and Chavez, Martin R.},
	title = {ChatGPT: a pioneering approach to complex prenatal differential diagnosis},
	year = {2023},
	journal = {American Journal of Obstetrics and Gynecology MFM},
	volume = {5},
	number = {8},
	doi = {10.1016/j.ajogmf.2023.101029},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162070624&doi=10.1016%2fj.ajogmf.2023.101029&partnerID=40&md5=dd71567de8b2acebbbf26d3aa8c34664},
	affiliations = {Division of Maternal-Fetal Medicine, Department of Obstetrics and Gynecology, NYU Langone Health, NYU Langone Hospital-Long Island, NYU Long Island School of Medicine, Mineola, NY (Drs Suhag and Kidd, Mses McGath and Cacace, and Dr Chavez), United States; Department of Clinical Genetics, NYU Langone Hospital-Long Island, Mineola, NY (Mses McGath and Cacace, and Dr Monteleone), United States; Department of Obstetrics and Gynecology, Richmond University Medical Center, Staten Island, NY (Dr Rajesh), United States; NYU Long Island School of Medicine, Mineola, NY (Mr Gelfinbein), United States},
	abstract = {This commentary examines how ChatGPT can assist healthcare teams in the prenatal diagnosis of rare and complex cases by creating a differential diagnoses based on deidentified clinical findings, while also acknowledging its limitations. © 2023},
	author_keywords = {artificial intelligence; ChatGPT-4; complex prenatal disorders; generative pre-trained transformer language model; human phenotype ontology; neonatal genetic disorders; online mendelian inheritance in man; prenatal diagnosis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}